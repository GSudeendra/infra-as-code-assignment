#!/bin/bash

set -e

# =============================================================================
# INFRASTRUCTURE DESTRUCTION SCRIPT
# =============================================================================
# Simple approach:
# 1. Try basic Terraform destroy first
# 2. If that fails, proceed with manual cleanup process
# 3. State bucket deletion (LAST - with confirmation)
# =============================================================================

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_separator() {
    echo "======================================"
}

# =============================================================================
# CONFIGURATION
# =============================================================================

# Get script and project directories
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

# AWS configuration
AWS_REGION="us-east-1"
STATE_BUCKET="iac-remote-state-160071257600-dev"
STATE_KEY="terraform.tfstate"
DYNAMO_TABLE="iac-remote-state-160071257600-dev"

# Resource lists for manual cleanup
BUCKETS=(
  "static-content-dev-916b5a00"
  "access-logs-dev-916b5a00"
  "iac-remote-state-logs-dev"
)

LOG_GROUPS=(
  "/aws/lambda/register_user-dev-916b5a00"
  "/aws/lambda/verify_user-dev-916b5a00"
  "/aws/apigateway/user-management-api"
)

# =============================================================================
# MANUAL CLEANUP FUNCTIONS
# =============================================================================

handle_terraform_state_mismatch() {
    local tf_output="$1"
    local operation="$2"
    
    if echo "$tf_output" | grep -qi "checksum.*state stored in S3.*does not match.*DynamoDB"; then
        print_warning "Terraform state/DynamoDB checksum mismatch detected for $operation"
        print_status "Attempting to resolve checksum mismatch..."
        
        # Delete DynamoDB lock/checksum
        aws dynamodb delete-item \
            --table-name "$DYNAMO_TABLE" \
            --key '{"LockID": {"S": "terraform.tfstate"}}' \
            --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete DynamoDB lock"
        
        # Delete S3 state file as last resort
        aws s3 rm "s3://$STATE_BUCKET/$STATE_KEY" 2>/dev/null || print_warning "Could not delete S3 state file"
        
        print_success "State mismatch resolved. Retrying terraform destroy..."
        return 0
    fi
    return 1
}

cleanup_s3_buckets() {
    print_status "Manual cleanup: Emptying and deleting S3 buckets..."
    
    for BUCKET in "${BUCKETS[@]}"; do
        if aws s3api head-bucket --bucket "$BUCKET" --region "$AWS_REGION" 2>/dev/null; then
            print_status "Emptying bucket: $BUCKET"
            aws s3 rm "s3://$BUCKET" --recursive 2>/dev/null || print_warning "Could not empty $BUCKET"
            
            print_status "Deleting bucket: $BUCKET"
            aws s3api delete-bucket --bucket "$BUCKET" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete $BUCKET"
            
            sleep 2  # Rate limiting
        else
            print_warning "Bucket $BUCKET does not exist or is already deleted"
        fi
    done
}

cleanup_cloudwatch_logs() {
    print_status "Manual cleanup: Deleting CloudWatch log groups..."
    
    for LOG_GROUP in "${LOG_GROUPS[@]}"; do
        if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP" --region "$AWS_REGION" 2>/dev/null | grep -q "$LOG_GROUP"; then
            print_status "Deleting log group: $LOG_GROUP"
            aws logs delete-log-group --log-group-name "$LOG_GROUP" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete $LOG_GROUP"
            sleep 1
        else
            print_warning "Log group $LOG_GROUP does not exist or is already deleted"
        fi
    done
}

cleanup_iam_resources() {
    print_status "Manual cleanup: Cleaning up IAM resources..."
    
    # Delete IAM OIDC Provider
    local oidc_arn
    oidc_arn=$(aws iam list-open-id-connect-providers --region "$AWS_REGION" --query 'OpenIDConnectProviderList[*].Arn' --output text 2>/dev/null | grep 'token.actions.githubusercontent.com' || true)
    
    if [ -n "$oidc_arn" ]; then
        print_status "Deleting IAM OIDC provider: $oidc_arn"
        aws iam delete-open-id-connect-provider --open-id-connect-provider-arn "$oidc_arn" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete OIDC provider"
    fi
    
    # Delete GitHub Actions Role
    local github_role="iac-github-actions-role-dev"
    if aws iam get-role --role-name "$github_role" --region "$AWS_REGION" &>/dev/null; then
        print_status "Cleaning up IAM role: $github_role"
        
        # Detach managed policies
        for policy_arn in $(aws iam list-attached-role-policies --role-name "$github_role" --region "$AWS_REGION" --query 'AttachedPolicies[*].PolicyArn' --output text 2>/dev/null); do
            print_status "Detaching policy $policy_arn from $github_role"
            aws iam detach-role-policy --role-name "$github_role" --policy-arn "$policy_arn" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not detach $policy_arn"
        done
        
        # Delete inline policies
        for policy_name in $(aws iam list-role-policies --role-name "$github_role" --region "$AWS_REGION" --query 'PolicyNames' --output text 2>/dev/null); do
            print_status "Deleting inline policy $policy_name from $github_role"
            aws iam delete-role-policy --role-name "$github_role" --policy-name "$policy_name" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete inline policy $policy_name"
        done
        
        # Delete the role
        aws iam delete-role --role-name "$github_role" --region "$AWS_REGION" 2>/dev/null && print_success "IAM role $github_role deleted" || print_warning "Could not delete IAM role $github_role"
    fi
}

cleanup_dynamodb_table() {
    print_status "Manual cleanup: Deleting DynamoDB table..."
    
    local table_name="iac-terraform-locks-dev"
    if aws dynamodb describe-table --table-name "$table_name" --region "$AWS_REGION" 2>/dev/null | grep -q 'TableDescription'; then
        print_status "Deleting DynamoDB table: $table_name"
        aws dynamodb delete-table --table-name "$table_name" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete DynamoDB table"
    fi
}

cleanup_backend_infrastructure() {
    print_status "Manual cleanup: Destroying backend infrastructure..."
    
    cd "$PROJECT_ROOT/infra/backend"
    
    # Initialize Terraform if needed
    if [ ! -d ".terraform" ]; then
        print_status "Initializing Terraform for backend..."
        terraform init
    fi
    
    # Try to destroy backend infrastructure
    set +e
    local tf_output
    tf_output=$(terraform destroy -auto-approve 2>&1)
    local tf_exit_code=$?
    set -e
    
    if [ $tf_exit_code -eq 0 ]; then
        print_success "Backend infrastructure destroyed via Terraform"
    else
        print_warning "Backend Terraform destroy failed, handling manually..."
        # Handle state mismatch if needed
        if handle_terraform_state_mismatch "$tf_output" "backend"; then
            # Retry once more
            set +e
            terraform destroy -auto-approve 2>/dev/null
            local retry_exit=$?
            set -e
            
            if [ $retry_exit -eq 0 ]; then
                print_success "Backend infrastructure destroyed via Terraform (retry)"
            else
                print_warning "Backend Terraform destroy failed even after retry"
            fi
        fi
    fi
    
    cd "$PROJECT_ROOT"
}

# =============================================================================
# MAIN EXECUTION
# =============================================================================

echo "üóëÔ∏è  Starting Infrastructure Destruction"
print_separator

# Verify AWS CLI configuration
print_status "Verifying AWS CLI configuration..."
if aws sts get-caller-identity &>/dev/null; then
    print_success "AWS CLI configured ‚úì"
else
    print_error "AWS CLI is not configured. Please configure AWS CLI first."
    exit 1
fi

# =============================================================================
# STEP 1: TRY SIMPLE TERRAFORM DESTROY FIRST
# =============================================================================

print_separator
print_status "STEP 1: Attempting simple Terraform destroy"
print_separator

cd "$PROJECT_ROOT/infra"

# Initialize Terraform if needed
if [ ! -d ".terraform" ]; then
    print_status "Initializing Terraform..."
    terraform init
fi

# Try the simple approach first
print_status "Executing: terraform destroy -auto-approve"
set +e  # Don't exit on error
terraform destroy -auto-approve
TERRAFORM_EXIT_CODE=$?
set -e  # Re-enable exit on error

if [ $TERRAFORM_EXIT_CODE -eq 0 ]; then
    print_success "‚úÖ Simple Terraform destroy succeeded!"
    print_status "Main infrastructure destroyed successfully"
    TERRAFORM_SUCCESS=true
else
    print_error "‚ùå Simple Terraform destroy failed"
    print_warning "Proceeding with manual cleanup process..."
    TERRAFORM_SUCCESS=false
fi

cd "$PROJECT_ROOT"

# =============================================================================
# STEP 2: MANUAL CLEANUP PROCESS (Only if Terraform failed)
# =============================================================================

if [ "$TERRAFORM_SUCCESS" = false ]; then
    print_separator
    print_status "STEP 2: Manual cleanup process (Terraform failed)"
    print_separator
    
    print_warning "Starting comprehensive manual cleanup..."
    
    # Try to handle state mismatch and retry Terraform
    print_status "Attempting to resolve potential state issues..."
    cd "$PROJECT_ROOT/infra"
    
    set +e
    local tf_output
    tf_output=$(terraform destroy -auto-approve 2>&1)
    local tf_retry_exit=$?
    set -e
    
    if [ $tf_retry_exit -eq 0 ]; then
        print_success "Terraform destroy succeeded after state cleanup"
    else
        if handle_terraform_state_mismatch "$tf_output" "main infrastructure"; then
            # Try one more time
            set +e
            terraform destroy -auto-approve 2>/dev/null
            local final_retry_exit=$?
            set -e
            
            if [ $final_retry_exit -eq 0 ]; then
                print_success "Terraform destroy succeeded after resolving state mismatch"
            else
                print_warning "Terraform destroy still failing, proceeding with manual resource cleanup..."
                
                # Manual cleanup of individual resources
                cd "$PROJECT_ROOT"
                cleanup_s3_buckets
                cleanup_cloudwatch_logs
                cleanup_iam_resources
            fi
        else
            print_warning "No state mismatch detected, proceeding with manual resource cleanup..."
            
            # Manual cleanup of individual resources
            cd "$PROJECT_ROOT"
            cleanup_s3_buckets
            cleanup_cloudwatch_logs
            cleanup_iam_resources
        fi
    fi
    
    cd "$PROJECT_ROOT"
    
    # Clean up backend infrastructure
    cleanup_backend_infrastructure
    
    # Clean up remaining backend resources
    cleanup_dynamodb_table
    
    print_success "Manual cleanup process completed"
else
    print_status "Skipping manual cleanup - Terraform destroy was successful"
    
    # Still need to clean up backend infrastructure
    print_status "Cleaning up backend infrastructure..."
    cleanup_backend_infrastructure
fi

# =============================================================================
# STEP 3: CLEANUP STATE ARTIFACTS
# =============================================================================

print_separator
print_status "STEP 3: Cleaning up state artifacts"
print_separator

# Clean up state file
if aws s3api head-object --bucket "$STATE_BUCKET" --key "$STATE_KEY" --region "$AWS_REGION" 2>/dev/null; then
    print_status "Deleting state file from S3..."
    aws s3 rm "s3://$STATE_BUCKET/$STATE_KEY" 2>/dev/null || print_warning "Could not delete state file"
    print_success "State file deleted ‚úì"
else
    print_warning "State file does not exist in S3"
fi

# Clean up DynamoDB lock
if aws dynamodb get-item --table-name "$DYNAMO_TABLE" --key '{"LockID": {"S": "terraform.tfstate"}}' --region "$AWS_REGION" 2>/dev/null | grep -q 'Item'; then
    print_status "Deleting DynamoDB lock/checksum..."
    aws dynamodb delete-item --table-name "$DYNAMO_TABLE" --key '{"LockID": {"S": "terraform.tfstate"}}' --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete DynamoDB lock"
    print_success "DynamoDB lock/checksum deleted ‚úì"
else
    print_warning "DynamoDB lock/checksum does not exist"
fi

# =============================================================================
# FINAL STEP: STATE BUCKET DELETION (LAST - WITH CONFIRMATION)
# =============================================================================

print_separator
print_status "FINAL STEP: State Bucket Deletion (REQUIRES CONFIRMATION)"
print_separator

print_warning "The state bucket contains the Terraform state and should be deleted LAST"
print_warning "This is a destructive operation that cannot be undone"
echo

# Prompt for confirmation
read -p "Do you want to delete the state bucket '$STATE_BUCKET'? (yes/no): " DELETE_STATE_BUCKET

if [ "$DELETE_STATE_BUCKET" = "yes" ]; then
    if aws s3api head-bucket --bucket "$STATE_BUCKET" --region "$AWS_REGION" 2>/dev/null; then
        print_status "Emptying state bucket: $STATE_BUCKET"
        aws s3 rm "s3://$STATE_BUCKET" --recursive 2>/dev/null || print_warning "Could not empty state bucket"
        
        print_status "Deleting state bucket: $STATE_BUCKET"
        aws s3api delete-bucket --bucket "$STATE_BUCKET" --region "$AWS_REGION" 2>/dev/null || print_warning "Could not delete state bucket"
        
        print_success "State bucket deleted ‚úì"
    else
        print_warning "State bucket does not exist or is already deleted"
    fi
else
    print_status "State bucket '$STATE_BUCKET' was NOT deleted (user chose 'no')"
    print_warning "Note: State bucket still exists and may incur storage costs"
fi

# =============================================================================
# COMPLETION
# =============================================================================

print_separator
print_success "üéâ Infrastructure destruction completed!"
print_separator

if [ "$TERRAFORM_SUCCESS" = true ]; then
    print_success "‚úÖ Clean destruction: Terraform destroy worked on first try"
else
    print_warning "‚ö†Ô∏è  Complex destruction: Required manual cleanup process"
fi

if [ "$DELETE_STATE_BUCKET" = "yes" ]; then
    print_success "‚úÖ Complete cleanup: All resources including state bucket destroyed"
else
    print_warning "‚ö†Ô∏è  Partial cleanup: State bucket remains (delete manually if needed)"
fi

print_status "AWS account is ready for fresh deployment"
print_separator